{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1a50ff",
   "metadata": {},
   "source": [
    "# **Project Description:Customer Feedback Sentiment Predictor**\n",
    " \n",
    "# **Data Description**:\n",
    "\n",
    "- A sentiment analysis job about the customer feedback\n",
    "- Feedback talks about different IT Services, Infrastructure etc.\n",
    "\n",
    "# **Dataset**:\n",
    "\n",
    "- Contains two columns \"review\" & \"label\"\n",
    "    - review : Customer Feedback about the Product and the Service\n",
    "    - label : '1' for Negative and '0' for Positive\n",
    "\n",
    "# **Objective**:\n",
    "\n",
    "- To classify the sentiment of customer reviews into the positive or negative, with negative sentiments being in focus\n",
    "\n",
    "# **Steps Applied**:\n",
    "\n",
    "- Basic Text pre-processing.\n",
    "\n",
    "- Build the classification model.\n",
    "    - HF BERT Based Models\n",
    "    - Fine Tuning based on Feedback Data\n",
    "\n",
    "- Tune & Evaluate the Model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ad2f0117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anchitsaxena/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anchitsaxena/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anchitsaxena/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata                          # Import Regex, string and unicodedata.\n",
    "import contractions                                     # Import contractions library.\n",
    "from bs4 import BeautifulSoup                           # Import BeautifulSoup.\n",
    "\n",
    "import numpy as np                                      # Import numpy.\n",
    "import pandas as pd                                     # Import pandas.\n",
    "import nltk                                             # Import Natural Language Tool-Kit.\n",
    "\n",
    "nltk.download('stopwords')                              # Download Stopwords.\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# NLP Related Dependencies\n",
    "from nltk.corpus import stopwords                       # Import stopwords.\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer.\n",
    "import matplotlib.pyplot as plt                         \n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "# Evaluation Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# TRANSFER LEARNING BASED LIBRARIES\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# Logging all the Actions.\n",
    "import logging                                           \n",
    "logging.basicConfig(filename='sentiment_analyzer_transfer_learning.log', \\\n",
    "                    encoding='UTF-8', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a20a269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_analyze_data(datafile):\n",
    "    \"\"\"\n",
    "    Read the Data File and Log the Dimension\n",
    "    Count and Drop Null Values\n",
    "    \n",
    "    datafile : Source Data File\n",
    "    data : Returns non-null df\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(datafile)\n",
    "    logging.info(str(data.shape))\n",
    "    logging.info(str(data.isnull().sum(axis=0)))\n",
    "    data.dropna(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47a97a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_analyze_data('review_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b471656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_org = pd.read_csv('review_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ce9ec",
   "metadata": {},
   "source": [
    "# Data Pre-processing:\n",
    "- Remove html tags.\n",
    "- Replace contractions in string. (e.g. replace I'm --> I am) and so on.\\\n",
    "- Remove numbers.\n",
    "- Tokenization\n",
    "- To remove Stopwords.\n",
    "- Lemmatized data\n",
    "- We have used NLTK library to tokenize words , remove stopwords and lemmatize the remaining words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "790c44ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    \"\"\"\n",
    "    Remove HTML Tags, if any\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a05041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94bc508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    \"\"\"Remove Numbers from the string of text\"\"\"\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f438ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    data['review'] = data.apply(lambda row: nltk.word_tokenize(row['review']), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a59b0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'] = data['review'].apply(lambda x: strip_html(str(x)))\n",
    "data['review'] = data['review'].apply(lambda x: replace_contractions(x))\n",
    "data['review'] = data['review'].apply(lambda x: remove_numbers(x))\n",
    "data = tokenize_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df5093b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords():\n",
    "    stopwords_ls = stopwords.words('english')\n",
    "\n",
    "    customlist = ['not', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
    "        \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
    "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "        \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "    # Set custom stop-word's list as not, couldn't etc. words matter in Sentiment, \n",
    "    # so not removing them from original data.\n",
    "\n",
    "    stopwords_mod = list(set(stopwords_ls) - set(customlist))\n",
    "    return stopwords_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dbdc1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = filter_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bcedf6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_list(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67d88eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize_list(words)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a981ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'] = data.apply(lambda row: normalize(row['review']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c0cfe3",
   "metadata": {},
   "source": [
    "### Model evaluation criterion\n",
    "\n",
    "### Model can make wrong predictions as:\n",
    "\n",
    "1. Predicting a Review/Feedback being Negative, but in reality it is Positive. \n",
    "2. Predicting a Review/Feednack being Positive, but in actuality it is Negative\n",
    "\n",
    "### Which case is more important? \n",
    "* Both the cases are important as:\n",
    "\n",
    "* If a positive Feedback is termed as Negative, it's a loss of resource. False Positive\n",
    "\n",
    "* If a Negative Feedback is termed as Positive, it's a loss of opportunity which in turn will result high Churn Rate. False Negative \n",
    "\n",
    "\n",
    "\n",
    "### How to reduce the losses?\n",
    "\n",
    "* Product would want `F1 Score` to be maximized, greater the F1  score higher are the chances of minimizing False Negatives and False Positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "218812c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline(pipeline_name, model_name):\n",
    "    \"\"\"\n",
    "    Load the \"Sentiment-Analysis\" Pipeline\n",
    "    and the corresponding model\n",
    "    \n",
    "    pipeline_name : Pipeline Utility to be used\n",
    "    model_name : Specific Model Name\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    sentiment_pipeline = pipeline(pipeline_name, \n",
    "                     model = model_name,\n",
    "                     tokenizer = tokenizer)\n",
    "    return sentiment_pipeline, tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d8128c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd6a0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline, tokenizer = load_pipeline('sentiment-analysis', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44d840d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, test_ratio):\n",
    "    \"\"\"\n",
    "    Split the Data into Independent and Target variable\n",
    "    \"\"\"\n",
    "    X = data.review\n",
    "    y = data.label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f79f12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(data, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441182f2",
   "metadata": {},
   "source": [
    "### FEW DATA CHALLENGES\n",
    "- Few of the Feedbacks are too large. Max Tensor size for BERT is 512\n",
    "- Encountered few size related exceptions and that's why applying Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3dba54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model = \"facebook/bart-large-cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0dcee490",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_pipeline, tokenizer = load_pipeline('summarization', summary_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58f28d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_for_longer_texts(X_train, max_size, min_size, summary_pipeline, tokenizer):\n",
    "    \"\"\"\n",
    "    Extract only the problematic Reviews with >= 3000 length and apply Summarizer\n",
    "    \n",
    "    X_train : Train set containing reviews\n",
    "    max_size : Maximum words to be in Summary\n",
    "    min_size : Minimum words to be in Summary\n",
    "    summary_pipeline : Pipeline Summary Instance\n",
    "    \"\"\"\n",
    "    train_samples = X_train.tolist()\n",
    "    index_ls = [i for i, x in enumerate(train_samples) if len(x) >= 3000]\n",
    "    counter = 0\n",
    "    for index in index_ls:\n",
    "        sample = train_samples[index]\n",
    "        if len(sample) >= 3000:\n",
    "            \"\"\"\n",
    "            SINCE SEQ2SEQ BERT HAVE ONLY 512 MAX LENGTH\n",
    "            \"\"\"\n",
    "            sample = sample[:512]\n",
    "            summary = summary_pipeline(sample, \\\n",
    "                max_length=max_size, min_length=min_size)\n",
    "\n",
    "            train_samples[index] = summary[0]['summary_text']\n",
    "    return train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e2b53d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but you input_length is only 84. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Your max_length is set to 100, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 100, but you input_length is only 90. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Your max_length is set to 100, but you input_length is only 85. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Your max_length is set to 100, but you input_length is only 88. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 74. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 100, but you input_length is only 87. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Your max_length is set to 100, but you input_length is only 70. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Your max_length is set to 100, but you input_length is only 88. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 78. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Your max_length is set to 100, but you input_length is only 74. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 100, but you input_length is only 88. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 91. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Your max_length is set to 100, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 100, but you input_length is only 94. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Your max_length is set to 100, but you input_length is only 89. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 85. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Your max_length is set to 100, but you input_length is only 96. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 100, but you input_length is only 89. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 94. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Your max_length is set to 100, but you input_length is only 77. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n",
      "Your max_length is set to 100, but you input_length is only 78. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Your max_length is set to 100, but you input_length is only 89. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 100, but you input_length is only 89. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 89. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 89. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 81. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 100, but you input_length is only 87. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Your max_length is set to 100, but you input_length is only 94. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Your max_length is set to 100, but you input_length is only 90. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Your max_length is set to 100, but you input_length is only 88. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 71. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Your max_length is set to 100, but you input_length is only 87. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Your max_length is set to 100, but you input_length is only 89. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 87. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Your max_length is set to 100, but you input_length is only 86. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Your max_length is set to 100, but you input_length is only 89. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 78. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Your max_length is set to 100, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 100, but you input_length is only 89. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 77. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n",
      "Your max_length is set to 100, but you input_length is only 78. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Your max_length is set to 100, but you input_length is only 89. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 100, but you input_length is only 88. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 90. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Your max_length is set to 100, but you input_length is only 90. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Your max_length is set to 100, but you input_length is only 85. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Your max_length is set to 100, but you input_length is only 79. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Your max_length is set to 100, but you input_length is only 86. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n"
     ]
    }
   ],
   "source": [
    "X_train_samples = get_summary_for_longer_texts(X_train, 100, 50, summary_pipeline, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6a96c4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but you input_length is only 91. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Your max_length is set to 100, but you input_length is only 85. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Your max_length is set to 100, but you input_length is only 84. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Your max_length is set to 100, but you input_length is only 81. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 100, but you input_length is only 94. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Your max_length is set to 100, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 100, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 100, but you input_length is only 96. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 100, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 100, but you input_length is only 94. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Your max_length is set to 100, but you input_length is only 95. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Your max_length is set to 100, but you input_length is only 78. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Your max_length is set to 100, but you input_length is only 84. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Your max_length is set to 100, but you input_length is only 78. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Your max_length is set to 100, but you input_length is only 81. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 100, but you input_length is only 90. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Your max_length is set to 100, but you input_length is only 88. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but you input_length is only 84. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Your max_length is set to 100, but you input_length is only 78. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Your max_length is set to 100, but you input_length is only 88. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n"
     ]
    }
   ],
   "source": [
    "X_test_samples = get_summary_for_longer_texts(X_test, 100, 50, summary_pipeline, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50648d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_sentiment_analysis(samples, sentiment_pipeline, label_map):\n",
    "    \"\"\"\n",
    "    Run Sentiment Analyzer on DistillBERT\n",
    "    \"\"\"\n",
    "    pred_ls = []\n",
    "    for sample in samples:\n",
    "        \"\"\"\n",
    "        SINCE SEQ2SEQ BERT HAVE ONLY 512 MAX LENGTH\n",
    "        \"\"\"\n",
    "        sample = sample[:512]\n",
    "        pred_ls.append(label_map[sentiment_pipeline(sample)[0]['label']])\n",
    "    return pred_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "07ddfc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_train, y_pred):\n",
    "    \"\"\"\n",
    "    Print F1 Score and the Classification Report\n",
    "    \"\"\"\n",
    "    print('F1-Score %s' % f1_score(y_pred, y_train))\n",
    "    logging.info('F1-Score %s' % f1_score(y_pred, y_train))\n",
    "    print(classification_report(y_train, y_pred,target_names=['Positive', 'Negative']))\n",
    "    logging.info(classification_report(y_train, y_pred,target_names=['Positive', 'Negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed9ba4",
   "metadata": {},
   "source": [
    "### Making Predictions on Summarized Feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4145f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls_summ = perform_sentiment_analysis(X_train_samples, sentiment_pipeline, {'POSITIVE':0, 'NEGATIVE':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e1eea",
   "metadata": {},
   "source": [
    "### Training Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "56ddf00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 0.06435944140862174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.99      0.41      0.58      2615\n",
      "    Negative       0.03      0.87      0.06        61\n",
      "\n",
      "    accuracy                           0.42      2676\n",
      "   macro avg       0.51      0.64      0.32      2676\n",
      "weighted avg       0.97      0.42      0.57      2676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(y_train, pred_ls_summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985793e9",
   "metadata": {},
   "source": [
    "### Test Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "34abf724",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls_test_summ = perform_sentiment_analysis(X_test_samples, sentiment_pipeline, {'POSITIVE':0, 'NEGATIVE':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d32617ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 0.06423357664233577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.99      0.43      0.60      1122\n",
      "    Negative       0.03      0.85      0.06        26\n",
      "\n",
      "    accuracy                           0.44      1148\n",
      "   macro avg       0.51      0.64      0.33      1148\n",
      "weighted avg       0.97      0.44      0.59      1148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(y_test, pred_ls_test_summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a48272f",
   "metadata": {},
   "source": [
    "### Making Predictions on Text Preprocessed Data (including Lemmatization) Feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ecd5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls_pre = perform_sentiment_analysis(X_train.tolist(), sentiment_pipeline, {'POSITIVE':0, 'NEGATIVE':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c01e5668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 0.06432038834951456\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.99      0.41      0.58      2615\n",
      "    Negative       0.03      0.87      0.06        61\n",
      "\n",
      "    accuracy                           0.42      2676\n",
      "   macro avg       0.51      0.64      0.32      2676\n",
      "weighted avg       0.97      0.42      0.57      2676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(y_train, pred_ls_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b57fcae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls_test_pre = perform_sentiment_analysis(X_test.tolist(), sentiment_pipeline, {'POSITIVE':0, 'NEGATIVE':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "13fa3c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 0.0641399416909621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.99      0.43      0.60      1122\n",
      "    Negative       0.03      0.85      0.06        26\n",
      "\n",
      "    accuracy                           0.44      1148\n",
      "   macro avg       0.51      0.64      0.33      1148\n",
      "weighted avg       0.97      0.44      0.59      1148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(y_test, pred_ls_test_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61961ec0",
   "metadata": {},
   "source": [
    "### WITHOUT TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "18d06cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_org = data_org.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a26793b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w_norm, X_test_w_norm, y_train_w_norm, y_test_w_norm = split_data(data_org, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "21dda45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls_w_norm = perform_sentiment_analysis(X_train_w_norm.tolist(), \\\n",
    "                                sentiment_pipeline, {'POSITIVE':0, 'NEGATIVE':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b79ed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 0.10478359908883828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.99      0.71      0.82      2615\n",
      "    Negative       0.06      0.75      0.10        61\n",
      "\n",
      "    accuracy                           0.71      2676\n",
      "   macro avg       0.52      0.73      0.46      2676\n",
      "weighted avg       0.97      0.71      0.81      2676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(y_train, pred_ls_w_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fdf460b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls_test_w_norm = perform_sentiment_analysis(X_test_w_norm.tolist(), \\\n",
    "                                sentiment_pipeline, {'POSITIVE':0, 'NEGATIVE':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e7217b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 0.10382513661202186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.99      0.71      0.83      1122\n",
      "    Negative       0.06      0.73      0.10        26\n",
      "\n",
      "    accuracy                           0.71      1148\n",
      "   macro avg       0.52      0.72      0.47      1148\n",
      "weighted avg       0.97      0.71      0.81      1148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(y_test, pred_ls_test_w_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78200e",
   "metadata": {},
   "source": [
    "### Text Preprocessing Changes\n",
    "- Strip HTML Tags\n",
    "- Remove Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "33f4fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "    norm_docs = []\n",
    "    for doc in tqdm.tqdm(docs):\n",
    "        doc = strip_html_tags(doc)\n",
    "        doc = str(doc)\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        doc = doc.lower()\n",
    "        doc = remove_accented_chars(doc)\n",
    "        doc = contractions.fix(doc)\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()  \n",
    "        norm_docs.append(doc)\n",
    "    return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3b5a3319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 2676/2676 [00:00<00:00, 9359.65it/s]\n",
      "100%|███████████████████████████████████| 1148/1148 [00:00<00:00, 10054.57it/s]\n"
     ]
    }
   ],
   "source": [
    "norm_train_reviews = pre_process_corpus(X_train_w_norm)\n",
    "norm_test_reviews = pre_process_corpus(X_test_w_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "49412cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls_norm = perform_sentiment_analysis(norm_train_reviews, sentiment_pipeline, {'POSITIVE':0, 'NEGATIVE':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "79aca19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 0.08742194469223906\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.99      0.61      0.76      2615\n",
      "    Negative       0.05      0.80      0.09        61\n",
      "\n",
      "    accuracy                           0.62      2676\n",
      "   macro avg       0.52      0.71      0.42      2676\n",
      "weighted avg       0.97      0.62      0.74      2676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(y_train, pred_ls_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "67d15392",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls_test_norm = perform_sentiment_analysis(norm_test_reviews, sentiment_pipeline, {'POSITIVE':0, 'NEGATIVE':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2ca34472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 0.08869179600886919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.99      0.64      0.78      1122\n",
      "    Negative       0.05      0.77      0.09        26\n",
      "\n",
      "    accuracy                           0.64      1148\n",
      "   macro avg       0.52      0.70      0.43      1148\n",
      "weighted avg       0.97      0.64      0.76      1148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(y_test, pred_ls_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4d139",
   "metadata": {},
   "source": [
    "### LOADING FINE-TUNE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202e1a1",
   "metadata": {},
   "source": [
    "### Hosted Model at HuggingFace Github repository\n",
    "- https://huggingface.co/anchit48/fine-tuned-sentiment-analysis-customer-feedback/tree/main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3b2bf8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine_tuned = \"anchit48/fine-tuned-sentiment-analysis-customer-feedback\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7df912da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6fe98e087e4f95969d2f6594a35a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/812 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85f007b8f98488e82b3ef71727ca1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbd3931667144e295b4ab466687f6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/405 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020d766290184e328b0bc95c419691b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129007c6ac394f15aaebc1814c288e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26308d159afa4d69bc69fd9e56f8cc6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_pipeline_fine = pipeline(\"sentiment-analysis\", model=model_fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d9747302",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fine_tune = perform_sentiment_analysis(X_train_w_norm.tolist(), sentiment_pipeline_fine, {'POSITIVE':0, 'NEGATIVE':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8bcaf2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 0.5252525252525253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.99      1.00      0.99      2615\n",
      "    Negative       0.68      0.43      0.53        61\n",
      "\n",
      "    accuracy                           0.98      2676\n",
      "   macro avg       0.84      0.71      0.76      2676\n",
      "weighted avg       0.98      0.98      0.98      2676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(y_train_w_norm, y_pred_fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e3722b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_fine_tune = perform_sentiment_analysis(X_test_w_norm.tolist(), \\\n",
    "                            sentiment_pipeline_fine, {'POSITIVE':0, 'NEGATIVE':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a8d4dbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 0.5217391304347826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.99      0.99      0.99      1122\n",
      "    Negative       0.60      0.46      0.52        26\n",
      "\n",
      "    accuracy                           0.98      1148\n",
      "   macro avg       0.79      0.73      0.76      1148\n",
      "weighted avg       0.98      0.98      0.98      1148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(y_test_w_norm, y_pred_test_fine_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f1809",
   "metadata": {},
   "source": [
    "### Analyzing Negative Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2111a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_neg_reviews(y_test, data_org):\n",
    "    \"\"\"\n",
    "    Spot Checks on few Negative Samples\n",
    "    \"\"\"\n",
    "    index_ls = [i for i, num in enumerate(y_test) if num == 1]\n",
    "    neg_reviews = [(idx, data_org['review'].tolist()[idx]) for idx in index_ls]\n",
    "    return neg_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d5c29423",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reviews = extract_neg_reviews(y_test, data_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "52498a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14,\n",
       "  \"Overview\\n\\nThis is a great array for someone looking to expand their home storage network or possibly someone with a SOHO setup that needs plenty of storage. I used this array to combine many different drives I had laying around from past system builds as well as to give me a central place to go for over 15TB of storage.\\n\\nUnpacking\\n\\nThe array arrived well packed with two layers of boxes and internal Styrofoam.\\n\\nInstallation\\n\\nYou will definitely want to buy the optional rackmount mounting rails. The railkit costs approximately $32 more but was worth not having to sit the array on top of another rackmount device. There is no way the front brackets will hold the weight of the array empty let alone full of hard drives.\\n\\nHardware\\n\\nAll hardware was included. eSATA cables, SATA cables, power cables, drive trays, and drive tray mounting screws were included. The chassis is reasonably high quality metal, and I believe the power supply is a standard ATX power supply which is always preferred over some custom form factor so that if the OE PS dies you can get another one from anywhere. The array has plenty of fans, so far I have 12 drives in the array and no cooling issues.\\n\\nThe array is surprisingly quiet even when fully populated with WD Caviar Greens. The fans are barely noticeable to include the power supply fan. The indicator lights on the front are sparse but do provide basic information such as which drive bays are filled, which drives are active, and which internal RAID controller is active. A nice to have would have been a LCD screen with temp and other environmental info but at this price point its not surprising that its lacking.\\n\\nOne HW feature that I found totally cool was that you can bring your own eSATA port multiplying RAID controller card if you wanted, disable the RAID controller in the array, and use all of your own features with your own RAID controller card.\\n\\nSoftware\\n\\nThe CD that came with mine was useless, trying to read it locked up two different laptops so I ended up downloading the latest software right from their website. The GUI interface for their drives and software is terrible, incredibly cheesy, and felt like something developed in the 90's, but it did install without a hitch and instantly recognized the attached array. Configuration of the RAID went without a hitch.\\n\\nI did all of my configurations from a server running Windows Server 8 BETA x64 so I can say its not only Windows Server 8 BETA compatible its also 64bit compatible driver software.\\n\\nA nice feature is that the HD's SMART monitoring information is available via the array's driver software.\\n\\nFeatures\\n\\nThe amount of storage options this unit provides is awesome. I was looking for a budget eSATA rackmount enclosure with at least 8 drive bays and this unit definitely performs above and beyond my expectations. It offers all the usual RAID levels but the best part is that internally it is comprised of 3 separate RAID controllers which are accessible via 3 separate eSATA ports on the back so if you wanted to you could connect 3 different servers to this array. Also, if you choose to connect a device to the array that does not have a port multiplier eSATA port, you can configure the internal controller of the array to do the port multiplication for you and present a single logical storage volume to the connected device.\\n\\nDuring my burn-in testing of my 2TB WDC Green RAID array I saw sustained writes of approximately 86MB/s and reads of approximately 98MB/s for a RAID 5 array of 4 1TB WDC Greens.\\n\\nThe Bad\\n\\nThe only bad thing about this array is the incredibly cheap hard drive carriages. You have to be very careful when mounting, inserting, or removing hard drives from this array. The carriages are very fragile, thin, plastic and will easily crack or break if handled roughly.  Once the drives are properly inserted into the array you should be ok.\\n\\nConclusion\\n\\nThis is a great budget minded eSATA RAID enclosure for anyone seeking a ton of drive bays, HW RAID controllers, and storage flexibilty for less than $1000.00. It certainly is not enterprise class, but then again, neither is the price.\"),\n",
       " (42,\n",
       "  'Update 3/4/15: My HP Pav. Elite  running Win7 Pro died and I was loaded this Win. 7 package, essentially resurrecting it.\\n\\nI purchased \"Win 7 Home Premium SP1 32 (OEM) System\" to upgrade an older HP laptop running Vista which I totally hated. And I\\'ve very little to say about the experience, except that I loaded the DVD, followed the screen prompts, and now the laptop is running Windows 7 which I like almost as much as XP.\\n\\nIt was as easy as I hoped and, honestly, I\\'m beside myself; wow! Microsoft actually got something right.'),\n",
       " (60, 'Came to me very quickly.\\nAll as in the offer.\\nGood value.'),\n",
       " (123,\n",
       "  'After a long search I was finally able to locate this modem that is not only a fax and data modem but also a voice modem.  Voice modem is essential for answering machine type software on my computer.  The product is working as expected.'),\n",
       " (348,\n",
       "  'I bought this to watch my 2 kittys while I am out for work. I check them on my iphone periodically. The image is good enough to meet my need both in bright and dark. The installation and set up were easy. I though I had problem and emailed cutomer service. they reponded immediately but I found it was a small issue and solve it before scheduled an phone appt with them. I bought the one with wireless adapter and love that I can leave it in my kitty room which is diffdernet room from my PC and wireless rooter are. Only one note is that you can communiate with speaker and microphone when you are on PC; but on iphone via Seedonk, it is speaker only, i.e., you can hear them but you cannot talk to them. This item really gave me a peace of my mind... thank you!'),\n",
       " (349, 'Arrived as described.  I was pleased with purchase'),\n",
       " (426, 'Works just fine'),\n",
       " (429,\n",
       "  \"If you don't like bells and whistles,  this is great, plenty of room for all conponents. Very good price too.\"),\n",
       " (441,\n",
       "  \"...TurboTax can help ease the pain.\\n\\nThis is the 15th consecutive year I've used TurboTax for our state and federal returns. Over that time, Intuit has offered some better (and weaker) versions. The 2013 edition is as good as any I've used to date.\\n\\nIf asked to identify those chores that personal computing and online services have helped tame, I wouldn't hesitate to place income tax preparation at the top of the list. Tax time before TurboTax used to be a three-day long parade of headache-inducing calculations, trips to the library for the correct forms, and the pain of the calculations from worksheets and schedules. While my tax situation has been relatively stable from year to year, TurboTax has compressed three days of extended misery over the correctness of my math to a much shorter period of compressed misery over the extent of my tax burden and interpreting IRS instructions. Intuit had rough period in the early 2000s with both privacy and functionality in TurboTax (and a STINGY approach to how many users could prepare taxes from a single copy). Happily, those days are far in the past\\n\\nThis version allows you to prepare and E-File up to five federal returns. Installation was issue free and took less than 10 minutes in a Windows 7 machine (64-bit, Intel i7-3770 CPU @ 3.4 GHZ and 16 GB RAM). Updates since installation have also been painless. Because I had return data from last year's edition on my computer already, our personal data was already filled in, and the program smartly queried about the same sources of income and interest and deductions/credits as last year.\\n\\nWith all of my tax documents at hand, and a moderately complex 1040 filing scenario, I was finished in less than an hour. Other than the fundamental pain that comes with tax time: what's not to like about that?\"),\n",
       " (473, \"good price and product just didn't need it.\"),\n",
       " (544,\n",
       "  'easy to use/install - amazon library helpful.\\n\\n10 unit pricing very good'),\n",
       " (614,\n",
       "  '<a data-hook=\"product-link-linked\" class=\"a-link-normal\" href=\"/Convert-media-files-MPEG-AVI-DivX-WMV-WMA-WAV-MOV-MP4-DVR-MS-ASF-VOB-MP3-WAV-Download/dp/B015OY6WKS/ref=cm_cr_arp_d_rvw_txt?ie=UTF8\">Convert media files: MPEG,AVI,DivX,WMV,WMA,WAV,MOV,MP4,DVR-MS,ASF,VOB,MP3,WAV [Download</a>]'),\n",
       " (617, 'Worked as advertised.'),\n",
       " (633,\n",
       "  \"I remember using this program a few years ago, but it was never this good. Like so many other reviewers I am using the program to make this review right now, and it's quite impressive. Although I don't have any disabilities, I have started to get carpal tunnel syndrome and this really helps to minimize repetitive motion.\\nIt does take a little getting used to especially with punctuation but the system is very smart and does not require you to talk like a robot. As long as you enunciate the words reasonably well 99% of the time it will recognize the word correctly.\\n\\nThis is still the first day I'm trying this and I am very impressed and highly recommend this.\"),\n",
       " (646,\n",
       "  'this street atlas is very good\\nit works great on both Vista and XP\\nReally like the price and the value\\nthanks'),\n",
       " (705,\n",
       "  'Dont believe all the negativity we hear about windows 8, no one likes change but once you get used to the new interface its great, about time we started seeing touch screen operating systems. I found a great book on how to get started using windows 8&nbsp;<a data-hook=\"product-link-linked\" class=\"a-link-normal\" href=\"/Using-Windows-8-Computer-Training/dp/B00BCGTH7I/ref=cm_cr_arp_d_rvw_txt?ie=UTF8\">Using Windows 8 (Computer Training)</a>.\\n\\nOnce i got used to windows 8 i found it ran better than then previous versions.'),\n",
       " (816,\n",
       "  'I was a bit intimidated at first by this software suite, but once I installed it and followed the tutorials, I found it highly intuitive to use... and a lot of fun, too.'),\n",
       " (903,\n",
       "  \"Been using this for years.  Several newer products out there but this works so I'm sticking to it.  No reason to try something else.\"),\n",
       " (925,\n",
       "  \"Autodesk SketchBook Pro 2010 is quality paint/drawing software designed for use with a graphic tablet and pen.  However it will also work with a conventional mouse but getting quality results is more difficult that way.\\n\\nI installed in on a Windows XP SP3 computer along with a Bamboo Tablet/Pen and it works smoothly and with no problems.  I also tried it on a Vista Laptop but unfortunately Vista 64 bit is not supported yet so please take note.\\n\\nThe program is excellent, the interface is easy to use and with a flick of your pen you can access the many features including pencils, brushes, markers, layers and pan/zoom tools.\\n\\nThis software is so good that even people who have little drawing ability can soon get great results.  I would claim to be average at drawing but this program is so versatile and intuitive that I can get excellent results at times. It also increases my workflow because of the processing speed and easy accessibility of all the features.\\n\\nIt is difficult to compare it to other programs.  Sure there are other paint programs like Photoshop or Painter but nothing compares to the ease and flexibility of using this program and at $100 it is a fraction of the price of other drawing programs.\\n\\nIf you still want to use Photoshop, then draw with this one and enhance your picture with Photoshop.  This will save in most formats; it uses layered TIF images but it will also open/save BMP, GIF, JPG, PNG files and PSD files for use in Adobe Photoshop.\\n\\nI highly recommend this software, it doesn't need much selling because it is already award winning and used by a stream of respectable computer artists and designers.  All I can say is that if you have artistic ambitions and want a good program to enhance them then it is well worth a look at.\"),\n",
       " (929,\n",
       "  'I have been using this for about a year now, and as a novice video maker, I found this to be indispensable. I was able to quickly learn how to use it without any trouble and I have had no trouble using it since. It is robust enough that it grows with you as you improve your video skills but easy enough to use for a first time user. Highly recommended.'),\n",
       " (933, 'Nice program.'),\n",
       " (972, 'Downloaded right after purchase, and installed with no problem.'),\n",
       " (1001, 'Thanks.'),\n",
       " (1009,\n",
       "  'I am not going to beat around the bush here.  Windows 8 is the worst operating system I have ever had the misfortune of working with.  This is coming from someone that has used Windows ME and Vista.  I don\\'t know what Microsoft was thinking in releasing OS or if they were even thinking at all. But apparently they only care about the \"next big thing\", and care nothing about people who actually use their software.  They also apparently don\\'t have a QA department because I find it very hard to believe that any sane person would sign off on an OS with so many usability problems and bugs.  I don\\'t know where to begin but to get started I will just take excerpts from an opinion I wrote about Windows 8 on another website and go from there.\\n\\nWindows 8 was clearly designed for a touchscreen, but not every one wants touchscreen, and not every windows application is compatible with touch screen. Users should not be forced to buy a computer that has touch screen just so they can use Windows 8. Windows 8 might work well on a tablet or phone, but it sucks as a desktop OS however Microsoft seems hell bent on shoving touch screen down our throats.  If people wanted a tablet or a phone, then guess what?  They would BUY a tablet or a phone.  Why try to force an OS designed for tablets or phones onto a desktop or laptop?\\n\\nTo my surprise Windows 7 ended up being a pretty good OS and I use it at work and many people also like Windows 7. However Microsoft had to go and ASSUME (like they always do) that we all want touch screen now and remove everything that worked so well in previous OS\\'s.  i.e. the start button.  Pressing the windows key on Windows 8 will not bring up the start menu, but instead it will bring you to a poorly designed screen with stupid tiles every where making finding what you want a tedious task.  They hid everything from you and many times you have to search the web to get help to find what you are looking for.\\n\\nTouch screen is stupid for a desktop or laptop computer. The tiles, metro UI, or whatever you call it is idiotic and makes working in the OS an absolute nightmare. I couldn\\'t care less about stupid teeny bopper Facebook or other social media crap. I just want to do what I need to do on the computer and not have to go through menu after menu of Nickelodeon type tiles just do a simple task. This OS isn\\'t about productivity, its about pretty colors, and Facebook status updates. Microsoft seemed to forget people actually do WORK on their computers.  For example I VPN to my work network to work from home occasionally and I need remote desktop connection to connect to the many servers I use at work.  However because Windows 8 is so poorly designed, instead of just double clicking the mstsc application to open a new window, now double clicking it just brings up the existing connection, meaning I have to again search for the mstsc app, right click it and then click open new connection.  This takes a lot of time especially when I\\'m working on 3, 4, or 5 servers at the same time.  Even stuff like configuring a printer or getting into the control panel is an exercise in hair pulling frustration.  Switching between windows which what 99.999% of people who actually do WORK on a laptop do, is extremely cumbersome basically eliminating any hope of productivity.  Heck even turning the damn thing off, or logging a user off is made unnecessarily complicated. Who ever designed this OS or thought people would actually like this mess needs to be fired and banned from ever designing an OS again.\\n\\nLuckily the only computer in my household that had this turd of an operating system was my son\\'s laptop.  But low and behold Windows 8 crashed for the 2nd time and now won\\'t let me back in thanks to the endless \"We are automatically repairing your PC\". loop.  The automatic repair is just as useful as Window\\'s assistant in previous operating systems.  Now I can\\'t get into Windows at all so all the stuff is now gone.  Luckily I had the foresight not to put any important files on this computer so I really don\\'t lose anything.  I\\'m writing this review on my Windows XP laptop I bought nearly 6 years ago, and other than a few hardware issues, the operating system itself has been pretty solid, which is much more than I can say about Windows 8.  Today I\\'m going to install Windows XP on my son\\'s laptop and be done with it.  I should have done this as soon as my son got this laptop and I wouldn\\'t be going through this today.\\n\\nWindows 8 is the reason I will never buy another laptop unless it comes with Windows 7. In addition, I will never even consider purchasing a Windows Tablet or a Windows phone. Words simply can\\'t express how much I hate, loathe, despise, detest, abhor this operating system.  I can\\'t wait to get it off my son\\'s laptop and out of my life for good.'),\n",
       " (1083,\n",
       "  'The software is great for somebody who wants the ability to not be annoyed.  I hate even knowing I have this type of software let alone being notified about what it is doing. I have always been looking for a service to just run in the background, which is the best part about this software.  You can select how much interaction youll have (such as notifications).  For example, Ive set it up to be completely uninstructive unless I actually have a virus.  The default settings are kind of annoying, but you can easily disable unwanted features.  Id recommend this software for someone who wants their computer protected but never plans to do anything advanced like customizing their firewall.'),\n",
       " (1117, 'Good seller and ok item.')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43debd",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "- Clearly few of the Samples looks Positives like \"Thanks\", \"Nice Program\", \"Good seller and ok item.\", \"Downloaded right after purchase, and installed with no problem.\"\n",
    "- There needs to be a thorough look-up to the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253b4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
